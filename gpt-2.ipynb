{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8566d5cb",
   "metadata": {},
   "source": [
    "# Setting up the environment\n",
    "\n",
    "**Note:** Before starting, be sure to have installed the dependencies as explained in the README.md file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bb5b24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check device\n",
    "def get_device():\n",
    "    if torch.cuda.is_available(): \n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "# device = get_device()\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61ed491",
   "metadata": {},
   "source": [
    "Importing the gpt-2 specific tokenizer and model from the transformers library (`GPT2Tokenizer` and `GPT2LMHeadModel`). Virtually no difference from importing it from `AutoTokenizer`, `AutoModelForCausalLM`, except that the former is more specific and easier to use for accessing the model's specific methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7842330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Loading the model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Loading the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf0e46",
   "metadata": {},
   "source": [
    "# Understanding GPT-2 ðŸ’¬\n",
    "\n",
    "If we `print(model)`, we can actually see the full model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11180a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e56240",
   "metadata": {},
   "source": [
    "- `(wte)` or **Word Token Embeddings**: This layer converts the input tokens (e.g. \"apple\") into their corresponding embeddings (some vector of numbers). Sort of like a dictionary that maps tokens to their corresponding embeddings.\n",
    "\n",
    "> The output `Embedding(50257, 768)` means that the model has a vocabulary of 50,257 tokens (words), and each token is represented by a 768-dimensional vector.\n",
    "\n",
    "\n",
    "\n",
    "- `(wpe)` or **Word Position Embeddings**: This layer provides positional information to the tokens. It helps the model understand the order of tokens in the sequence. In Transformer models, the positional encoding is crucial because it provides information about the position of each token in the sequence, which is not taken into account otherwise.\n",
    "\n",
    "> The output `Embedding(1024, 768)` means that the model can handle sequences up to 1024 tokens, and each position in the sequence is represented by a 768-dimensional vector.\n",
    "\n",
    "\n",
    "\n",
    "- `(drop)` or **Dropout**: A regularization layer that randomly \"turns off\" some neurons during training. It forces the model to learn robust patterns rather than just memorizing the training data, this way avoiding overfitting.\n",
    "\n",
    "> The `p=0.1` means there is a 10% chance any given signal will be dropped during training.\n",
    "\n",
    "\n",
    "\n",
    "- `(h)` or **The Transformer Block Stack**: This is the \"body\" of the model. It contains the 12 identical layers stacked on top of each other that process the information sequentially.\n",
    "\n",
    "> The `(0-11): 12 x GPT2Block` indicates there are 12 distinct layers in this version of GPT-2.\n",
    "\n",
    "\n",
    "\n",
    "- `(ln_1)` or **Layer Normalization 1**: This layer normalizes the data (centering the numbers) before it enters the attention mechanism to keep calculations stable.\n",
    "\n",
    "> The `(768,)` confirms it preserves the standard vector size of the model.\n",
    "\n",
    "> The `eps=1e-05` is a small value added to the denominator to prevent division by zero.\n",
    "\n",
    "> The `elementwise_affine=True` allows the layer to learn a scaling factor and bias for each element in the input.\n",
    "\n",
    "\n",
    "\n",
    "- `(attn)` or **Masked Self-Attention**: This is the part of the model that looks at previous words in the sentence to figure out the context (e.g., figuring out if \"bank\" means a river or money).\n",
    "\n",
    "> The `c_attn` with `nf=2304` is interesting: it represents **3 x 768**. This layer creates the Query, Key, and Value vectors simultaneously for all attention heads. It is a `Conv1D(nf=2304, nx=768)` because...\n",
    "\n",
    "\n",
    "\n",
    "- `(mlp)` or **Feed-Forward Network**: After the attention mechanism gathers context, this simple neural network processes that information to extract meaning.\n",
    "\n",
    "> The `c_fc` with `nf=3072` shows that the model expands the data to be **4 times larger** (768 x 4 = 3072) to analyze complex patterns, before shrinking it back down with `c_proj`.\n",
    "\n",
    "\n",
    "\n",
    "- `(ln_f)` or **Final Layer Normalization**: The last cleanup step. It stabilizes the final output vectors coming out of the 12th layer before they are sent to the head.\n",
    "\n",
    "\n",
    "\n",
    "- `(lm_head)` or **Language Modeling Head**: The final classifier. This layer projects the model's internal \"thought\" (the 768-dim vector) back onto the full vocabulary to predict the next word.\n",
    "\n",
    "> The `(in_features=768, out_features=50257)` maps the internal hidden state back to a probability score for every single word in the 50,257-word dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0ccc02",
   "metadata": {},
   "source": [
    "## Comparing against other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c63e017",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "316716d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- BERT (Encoder-Only) Architecture ---\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"--- BERT (Encoder-Only) Architecture ---\")\n",
    "print(bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde0b2ad",
   "metadata": {},
   "source": [
    "### BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56526434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BART (Encoder-Decoder) Architecture ---\n",
      "BartForConditionalGeneration(\n",
      "  (model): BartModel(\n",
      "    (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
      "    (encoder): BartEncoder(\n",
      "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): BartDecoder(\n",
      "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "bart_tokenizer = AutoTokenizer.from_pretrained('facebook/bart-base')\n",
    "bart_model = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-base')\n",
    "\n",
    "print(\"\\n--- BART (Encoder-Decoder) Architecture ---\")\n",
    "print(bart_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299028f7",
   "metadata": {},
   "source": [
    "### T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e59d78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- T5 (Encoder-Decoder) Architecture ---\n",
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 512)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 8)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-5): 5 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 8)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-5): 5 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Model Name: 't5-small' is a compact version (~60M parameters)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "print(\"\\n--- T5 (Encoder-Decoder) Architecture ---\")\n",
    "print(t5_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b548c94",
   "metadata": {},
   "source": [
    "## Architectural Differences\n",
    "\n",
    "|Model|Architecture Type|QuickSummary|\n",
    "|------|-----------------|------------|\n",
    "| GPT-2|Decoder-Only|It's a single stack of 12 layers (`h`) designed for generating new text. It has no separate \"reading\" part. It just has the final `lm_head` to guess the next token.|\n",
    "| BERT|Encoder-Only|It's a single stack of 12 layers (`encoder`) designed for understanding input text. It has a special `pooler` to get a whole-sentence representation, but it can't easily generate new words.|\n",
    "|BART/T5|Encoder-Decoder|These have two full stacks: an `encoder` to process the input and a `decoder` to generate the output. Perfect for tasks where input and output lengths differ, like translation or summarization.|\n",
    "\n",
    "\n",
    "## Attention Mechanism Implementation\n",
    "\n",
    "- **GPT-2 (Masked Self-Attention)**: In the `(attn)` block, there is an internal causal mask applied. Any given token can only look at itself and the tokens that appeared before it in the input sequence. This basically means that it can only look at the tokens that appeared before it in the input sequence and not the future tokens; this is what makes it \"autoregressive\".\n",
    "\n",
    "- **BERT (Unmasked Self-Attention)**: The attention layer inside the `(encoder)` is just standard self-attention (no masking). Any token can look at every other token in the entire input sequence; both words before it and words after it. This deep, bidirectional context is why BERT is so strong at reading comprehension, sentiment analysis, and filling in the blanks compared to GPT-2.\n",
    "\n",
    "- **BART / T5 (Self- and Cross-Attention)**: BART and T5 have two full stacks: an `encoder` to process the input and a `decoder` to generate the output.\n",
    "\n",
    ">**Encoder**: Uses Unmasked Self-Attention (like BERT) to get a full, rich understanding of the input text.\n",
    "\n",
    ">**Decoder**: Uses Masked Self-Attention (like GPT-2) to generate the output word by word.\n",
    "\n",
    ">**Cross-Attention (`encoder_attn` in BART / `T5LayerCrossAttention` in T5)**: This layer sits between the Encoder's output and the Decoder's self-attention. It allows the decoder to look back at the entire context processed by the encoder, which is the key to linking the source text (e.g., a long article) to the generated output (e.g., a summary).\n",
    "\n",
    "## Advantages and Disadvantages\n",
    "\n",
    "|Model|Advantages|Disadvantages|\n",
    "|-----|----------|-------------|\n",
    "|GPT-2|Best-in-class for long-form, creative, and human-like text generation. It's a clean, efficient architecture for simple generation tasks.|Cannot see future context, which can sometimes lead to redundant or slightly less optimal word choices. Generating is slow because it's word-by-word.|\n",
    "|BERT|Deepest, most accurate contextual understanding. It's the go-to for classification, entity recognition, and question answering.|Cannot generate text on its own.|\n",
    "|BART/T5|Highly versatile for complex sequence tasks like summarization, machine translation, or data-to-text conversion. The encoder-decoder structure is robust.|They are generally more computationally expensive than the single-stack models because they run two full Transformer stacks for every generation task.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e20691d",
   "metadata": {},
   "source": [
    "# Generating Text with GPT-2 (Diverse Strategies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bb0d8e",
   "metadata": {},
   "source": [
    "`generate_text` is a function that generates text using the model. It is used to generate text with different strategies. Encodes the prompt, generates text using the model, and decodes the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec288304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length, **kwargs):\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=input_ids.shape[1] + max_length,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text[len(prompt):].strip()\n",
    "\n",
    "BASE_PROMPT = \"The future of artificial intelligence will involve\"\n",
    "MAX_NEW_TOKENS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc694e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Prompt: 'The future of artificial intelligence will involve'\n",
      "================================================================================\n",
      "A. Greedy Search:\n",
      "... a lot of work.\n",
      "\n",
      "\"We're going to have to see how we do it,\" said Dr. Michael S. Hirsch, a professor of computer science at the University of California, Berkeley. \"We're going to have to see\n",
      "\n",
      "B. Simple Sampling (Temp 1.0):\n",
      "... using the tools to get in the way of machines. If we continue to keep building more advanced machines, it will become more difficult to get in the way of machines or to get the full set of tools that are needed to create themâ€”it will\n"
     ]
    }
   ],
   "source": [
    "print(f\"Base Prompt: '{BASE_PROMPT}'\\n\" + \"=\"*80)\n",
    "\n",
    "# A. Greedy Search (No randomness: always pick the best word)\n",
    "greedy_output = generate_text(\n",
    "    BASE_PROMPT, \n",
    "    MAX_NEW_TOKENS, \n",
    "    do_sample=False, # Turns off sampling/randomness\n",
    ")\n",
    "print(f\"A. Greedy Search:\\n... {greedy_output}\\n\")\n",
    "\n",
    "\n",
    "# B. Simple Sampling (Temperature = 1.0)\n",
    "# We need do_sample=True to introduce randomness\n",
    "simple_sample_output = generate_text(\n",
    "    BASE_PROMPT, \n",
    "    MAX_NEW_TOKENS, \n",
    "    do_sample=True,\n",
    "    temperature=1.0\n",
    ")\n",
    "print(f\"B. Simple Sampling (Temp 1.0):\\n... {simple_sample_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b72db6c",
   "metadata": {},
   "source": [
    ">- **Greedy Search**: Always picks the most likely next token. Guarantees the most probable sequence and might get stuck in a loop because it cannot deviate from the most likely path.\n",
    "\n",
    ">- **Simple Sampling**: Introduces randomness by using a temperature parameter. This breaks the deterministic nature of greedy search and can lead to more diverse and creative outputs. Does not control where the randomness happens, risking incoherence if a low-probability, irrelevant word is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f869c4",
   "metadata": {},
   "source": [
    "## Temperature Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "860ab219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Temperature Control --- (Prompt: 'The future of artificial intelligence will involve')\n",
      "\n",
      "Temperature 0.5:\n",
      "... making more intelligent machines, and a lot of that will be driven by the rise of intelligent robots.\n",
      "\n",
      "What are your thoughts on this topic?\n",
      "\n",
      "I think it's a good question to ask, because I think it is a good question\n",
      "\n",
      "Temperature 1.0:\n",
      "... pushing humans into shiny new scenarios of how to predict our future behavior. While we'll likely learn much, our understanding of what computer is capable of is still very young.\n",
      "\n",
      "And replacing those new options should provide thinkers with fundamental finding a few years\n",
      "\n",
      "Temperature 1.5:\n",
      "... thousands less biological aging, Smith says, asserting further attest to Kokonomyidis fra tou olabel Kamemplain) who provided the aggro vig that revitalized Paraguolloashad smile https://monitorceez.net/SOitis-secNBA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Temperature Control --- (Prompt: '{BASE_PROMPT}')\\n\")\n",
    "\n",
    "# Experiment with different temperatures\n",
    "temperatures = [0.5, 1.0, 1.5]\n",
    "\n",
    "for t in temperatures:\n",
    "    output = generate_text(\n",
    "        BASE_PROMPT, \n",
    "        MAX_NEW_TOKENS, \n",
    "        do_sample=True,\n",
    "        temperature=t,\n",
    "        top_k=0,      # Disable top_k/top_p when only testing temperature\n",
    "        top_p=1.0\n",
    "    )\n",
    "    print(f\"Temperature {t}:\\n... {output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44520aeb",
   "metadata": {},
   "source": [
    "## Top-K Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7df50eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top-K Sampling Control --- (Prompt: 'The future of artificial intelligence will involve')\n",
      "\n",
      "Top-K 1 (Only consider top 1 words):\n",
      "... a lot of work.\n",
      "\n",
      "\"We're going to have to see how we do it,\" said Dr. Michael S. Hirsch, a professor of computer science at the University of California, Berkeley. \"We're going to have to see\n",
      "\n",
      "Top-K 50 (Only consider top 50 words):\n",
      "... a revolution in human intelligence in the way the world is governed.\"\n",
      "\n",
      "This raises significant questions about whether artificial intelligence will change the way we are governed and a lot of practical implications. The human brain is more like a machine than anything else. And\n",
      "\n",
      "Top-K 500 (Only consider top 500 words):\n",
      "... using any collection from any method (teaching or controlling) with any means instead of needing that genetic information from one specific application to a new, general purpose application; and\n",
      "\n",
      "2. other candidates must see that any of these methods will be a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Top-K Sampling Control --- (Prompt: '{BASE_PROMPT}')\\n\")\n",
    "\n",
    "# Experiment with different K values\n",
    "top_k_values = [1, 50, 500] \n",
    "\n",
    "for k in top_k_values:\n",
    "    # Set temperature to 1.0 (default randomness)\n",
    "    output = generate_text(\n",
    "        BASE_PROMPT, \n",
    "        MAX_NEW_TOKENS, \n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        top_k=k,\n",
    "        top_p=1.0      # Disable top_p\n",
    "    )\n",
    "    print(f\"Top-K {k} (Only consider top {k} words):\\n... {output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d23174a",
   "metadata": {},
   "source": [
    "## Top-P Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "569b8b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top-P (Nucleus) Sampling Control --- (Prompt: 'The future of artificial intelligence will involve')\n",
      "\n",
      "Top-P 0.95 (Nucleus sampling):\n",
      "... more than clinical studies. The challenges do lie in selecting the right ones to design new, better-researched systems, and evaluating when it emerges from those clinical study protocols, but research progress and automation will continue to lead.\n",
      "\n",
      "Top-P 0.75 (Nucleus sampling):\n",
      "... great costs and need to be solved before we can make any progress in this area.\n",
      "\n",
      "A comprehensive architecture of artificial intelligence would be essential to be successful and provide a bridge between those three points. The best architecture can be achieved when the science and\n",
      "\n",
      "Top-P 0.5 (Nucleus sampling):\n",
      "... more than just humans, but the entire human race as well.\n",
      "\n",
      "If you're a member of the Google team, you've probably heard of Google's \"Home Search.\" It's an incredibly simple and powerful way to find and search for information\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Top-P (Nucleus) Sampling Control --- (Prompt: '{BASE_PROMPT}')\\n\")\n",
    "\n",
    "# Experiment with different P values\n",
    "top_p_values = [0.95, 0.75, 0.5]\n",
    "\n",
    "for p in top_p_values:\n",
    "    output = generate_text(\n",
    "        BASE_PROMPT, \n",
    "        MAX_NEW_TOKENS, \n",
    "        do_sample=True,\n",
    "        temperature=1.0, # Standard randomness\n",
    "        top_k=0,         # Disable top_k\n",
    "        top_p=p\n",
    "    )\n",
    "    print(f\"Top-P {p} (Nucleus sampling):\\n... {output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfcb495",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
